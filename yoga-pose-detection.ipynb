{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yoga Pose Detection\n",
    "\n",
    "The project focuses on creating a Yoga Pose Detection system. The dataset has been created by me and comprises of 62 subclasses/ poses. Do perform this particular task, we can use two models, the initial one being ResNet50 and then next one is EfficientNetB0. Initially MobileNet and CNNs were tested out for this method but due to their poor accuracy during stage 1, the models have not been considered in this stage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Training with ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T13:11:16.313377Z",
     "iopub.status.busy": "2024-09-17T13:11:16.312396Z",
     "iopub.status.idle": "2024-09-17T13:27:30.840390Z",
     "shell.execute_reply": "2024-09-17T13:27:30.839308Z",
     "shell.execute_reply.started": "2024-09-17T13:11:16.313331Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50  # Using ResNet50\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "train_dir = \"/kaggle/input/yoga-pose-62/pose_62/train\"\n",
    "val_dir = \"/kaggle/input/yoga-pose-62/pose_62/val\"\n",
    "test_dir = \"/kaggle/input/yoga-pose-62/pose_62/test\"\n",
    "\n",
    "# Parameters\n",
    "img_height = 160  # Reduced size for faster processing\n",
    "img_width = 160   # Reduced size for faster processing\n",
    "batch_size = 32   # Reduced batch size\n",
    "num_classes = 62# Number of yoga pose classes\n",
    "epochs = 12      # Reduced number of epochs\n",
    "\n",
    "# Data Augmentation and Preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.resnet50.preprocess_input,\n",
    "    rotation_range=20,  # Adjusted augmentation range\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,  # Disabled vertical flip\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.resnet50.preprocess_input\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.resnet50.preprocess_input\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Building the Model with ResNet50\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "base_model.trainable = True\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True)\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs')\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=epochs,\n",
    "#     callbacks=[early_stopping, model_checkpoint, tensorboard_callback]\n",
    ")\n",
    "\n",
    "# Evaluate the Model\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Save the Model\n",
    "# model.save('yoga_pose_classifier.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Training with EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T15:27:16.795325Z",
     "iopub.status.busy": "2024-09-24T15:27:16.794437Z",
     "iopub.status.idle": "2024-09-24T15:39:21.147966Z",
     "shell.execute_reply": "2024-09-24T15:39:21.147010Z",
     "shell.execute_reply.started": "2024-09-24T15:27:16.795280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "train_dir = \"/kaggle/input/yoga-pose-62/pose_62/train\"\n",
    "val_dir = \"/kaggle/input/yoga-pose-62/pose_62/val\"\n",
    "test_dir = \"/kaggle/input/yoga-pose-62/pose_62/test\"\n",
    "\n",
    "# Parameters\n",
    "img_height = 160\n",
    "img_width = 160\n",
    "batch_size = 32\n",
    "num_classes = 62\n",
    "epochs = 10\n",
    "\n",
    "# Data Augmentation and Preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Building the Model with EfficientNetB0\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "base_model.trainable = True\n",
    "\n",
    "EfficientNet_model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "EfficientNet_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True)\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs')\n",
    "\n",
    "# Train the Model\n",
    "history = EfficientNet_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping, model_checkpoint, tensorboard_callback]\n",
    ")\n",
    "\n",
    "# Evaluate the Model\n",
    "test_loss, test_accuracy = EfficientNet_model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T15:45:56.236518Z",
     "iopub.status.busy": "2024-09-24T15:45:56.236114Z",
     "iopub.status.idle": "2024-09-24T15:45:56.869856Z",
     "shell.execute_reply": "2024-09-24T15:45:56.868779Z",
     "shell.execute_reply.started": "2024-09-24T15:45:56.236480Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EfficientNet_model.save('EfficientNetB0_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T15:47:19.953965Z",
     "iopub.status.busy": "2024-09-24T15:47:19.953543Z",
     "iopub.status.idle": "2024-09-24T15:47:19.961201Z",
     "shell.execute_reply": "2024-09-24T15:47:19.960265Z",
     "shell.execute_reply.started": "2024-09-24T15:47:19.953927Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "FileLink('/kaggle/working/EfficientNetB0_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Plotting Accuracy and Loss Graphs\n",
    "\n",
    "Since EfficientNetB0 gave better accuracy, we will now visualize the results. This can be done by plotting Accuracy and Loss Graphs for comparison between Train and Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T15:41:10.034592Z",
     "iopub.status.busy": "2024-09-24T15:41:10.034190Z",
     "iopub.status.idle": "2024-09-24T15:41:10.548721Z",
     "shell.execute_reply": "2024-09-24T15:41:10.547755Z",
     "shell.execute_reply.started": "2024-09-24T15:41:10.034555Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot accuracy and loss curves\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T15:41:16.814714Z",
     "iopub.status.busy": "2024-09-24T15:41:16.814318Z",
     "iopub.status.idle": "2024-09-24T15:41:16.822202Z",
     "shell.execute_reply": "2024-09-24T15:41:16.821390Z",
     "shell.execute_reply.started": "2024-09-24T15:41:16.814678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(test_generator.class_indices)  # Check this matches your actual classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T15:47:53.518058Z",
     "iopub.status.busy": "2024-09-24T15:47:53.517665Z",
     "iopub.status.idle": "2024-09-24T15:48:08.305913Z",
     "shell.execute_reply": "2024-09-24T15:48:08.304853Z",
     "shell.execute_reply.started": "2024-09-24T15:47:53.518021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the trained model (use the best saved model)\n",
    "model = load_model('EfficientNetB0_model.h5')\n",
    "\n",
    "# Get the test generator class indices\n",
    "labels_map = test_generator.class_indices\n",
    "labels_map = {v: k for k, v in labels_map.items()}  # Reverse to map indices to label names\n",
    "\n",
    "# Initialize lists to store data\n",
    "image_paths = []\n",
    "predicted_labels = []\n",
    "actual_labels = []\n",
    "checks = []\n",
    "\n",
    "# Loop through each batch in the test generator\n",
    "for i in range(len(test_generator)):\n",
    "    # Get the batch of images and corresponding labels\n",
    "    batch_images, batch_labels = test_generator[i]\n",
    "    \n",
    "    # Get the paths of the images in the batch\n",
    "    batch_image_paths = test_generator.filepaths[i*test_generator.batch_size:(i+1)*test_generator.batch_size]\n",
    "    \n",
    "    # Make predictions for the batch\n",
    "    batch_predictions = model.predict(batch_images)\n",
    "    batch_predicted_labels = np.argmax(batch_predictions, axis=1)  # Convert probabilities to label indices\n",
    "    batch_actual_labels = np.argmax(batch_labels, axis=1)  # Convert one-hot labels to indices\n",
    "    \n",
    "    # Store the path, predicted label, actual label, and check in lists\n",
    "    for j, img_path in enumerate(batch_image_paths):\n",
    "        image_paths.append(img_path)\n",
    "        \n",
    "        # Get the predicted and actual class names\n",
    "        predicted_label = labels_map[batch_predicted_labels[j]]\n",
    "        actual_label = labels_map[batch_actual_labels[j]]\n",
    "        predicted_labels.append(predicted_label)\n",
    "        actual_labels.append(actual_label)\n",
    "        \n",
    "        # Check if prediction is correct\n",
    "        check = 'True' if predicted_label == actual_label else 'False'\n",
    "        checks.append(check)\n",
    "\n",
    "# Create a DataFrame with the collected data\n",
    "df = pd.DataFrame({\n",
    "    'Image_Path': image_paths,\n",
    "    'Predicted_Label': predicted_labels,\n",
    "    'Actual_Label': actual_labels,\n",
    "    'Check': checks\n",
    "})\n",
    "\n",
    "# Print or save the DataFrame to a CSV file\n",
    "df.head()  # Show the first few rows\n",
    "#df.to_csv('prediction_results.csv', index=False)  # Save to CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T15:49:42.866183Z",
     "iopub.status.busy": "2024-09-24T15:49:42.865744Z",
     "iopub.status.idle": "2024-09-24T15:49:42.875734Z",
     "shell.execute_reply": "2024-09-24T15:49:42.874574Z",
     "shell.execute_reply.started": "2024-09-24T15:49:42.866145Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('/kaggle/working/prediction_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = EfficientNet_model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset- 52 Poses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T18:32:37.806532Z",
     "iopub.status.busy": "2024-11-05T18:32:37.805837Z",
     "iopub.status.idle": "2024-11-05T18:57:30.349325Z",
     "shell.execute_reply": "2024-11-05T18:57:30.348232Z",
     "shell.execute_reply.started": "2024-11-05T18:32:37.806493Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import ResNet50  # Using ResNet50\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "train_dir = \"/kaggle/input/yoga-pose-54/pose_54/train\"\n",
    "val_dir = \"/kaggle/input/yoga-pose-54/pose_54/val\"\n",
    "test_dir = \"/kaggle/input/yoga-pose-54/pose_54/test\"\n",
    "\n",
    "# Parameters\n",
    "img_height = 160  # Reduced size for faster processing\n",
    "img_width = 160   # Reduced size for faster processing\n",
    "batch_size = 32   # Reduced batch size\n",
    "num_classes = 54 # Number of yoga pose classes\n",
    "epochs = 21    # Reduced number of epochs\n",
    "\n",
    "# Data Augmentation and Preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.resnet50.preprocess_input,\n",
    "    rotation_range=20,  # Adjusted augmentation range\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,  # Disabled vertical flip\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.resnet50.preprocess_input\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.resnet50.preprocess_input\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Building the Model with ResNet50\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "base_model.trainable = True\n",
    "\n",
    "ResNet50_Model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "ResNet50_Model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True)\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs')\n",
    "\n",
    "# Train the Model\n",
    "history = ResNet50_Model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=epochs,\n",
    "#     callbacks=[early_stopping, model_checkpoint, tensorboard_callback]\n",
    ")\n",
    "\n",
    "# Evaluate the Model\n",
    "test_loss, test_accuracy = ResNet50_Model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Save the Model\n",
    "# model.save('yoga_pose_classifier.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNetB0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T18:58:46.915044Z",
     "iopub.status.busy": "2024-11-05T18:58:46.914632Z",
     "iopub.status.idle": "2024-11-05T19:15:55.499201Z",
     "shell.execute_reply": "2024-11-05T19:15:55.498165Z",
     "shell.execute_reply.started": "2024-11-05T18:58:46.915006Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "train_dir = \"/kaggle/input/yoga-pose-54/pose_54/train\"\n",
    "val_dir = \"/kaggle/input/yoga-pose-54/pose_54/val\"\n",
    "test_dir = \"/kaggle/input/yoga-pose-54/pose_54/test\"\n",
    "\n",
    "# Parameters\n",
    "img_height = 160\n",
    "img_width = 160\n",
    "batch_size = 32\n",
    "num_classes = 54\n",
    "epochs = 14\n",
    "\n",
    "# Data Augmentation and Preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Building the Model with EfficientNetB0\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "base_model.trainable = True\n",
    "\n",
    "EfficientNet_model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "EfficientNet_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True)\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs')\n",
    "\n",
    "# Train the Model\n",
    "history = EfficientNet_model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping, model_checkpoint, tensorboard_callback]\n",
    ")\n",
    "\n",
    "# Evaluate the Model\n",
    "test_loss, test_accuracy = EfficientNet_model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy and Loss Curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T19:35:15.783757Z",
     "iopub.status.busy": "2024-11-05T19:35:15.782814Z",
     "iopub.status.idle": "2024-11-05T19:35:16.461189Z",
     "shell.execute_reply": "2024-11-05T19:35:16.460260Z",
     "shell.execute_reply.started": "2024-11-05T19:35:15.783716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot accuracy and loss curves\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Prediction Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import tensorflow as tf\n",
    "\n",
    "def predict_and_plot_image(model_path, image_path, target_size, class_indices):\n",
    "    model = load_model(model_path)\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "\n",
    "    image = load_img(image_path, target_size=target_size)\n",
    "    image_array = img_to_array(image)\n",
    "\n",
    "    image_array = tf.keras.applications.efficientnet.preprocess_input(image_array)\n",
    "    image_array = np.expand_dims(image_array, axis=0)  # Add batch dimension\n",
    "    print(f\"Image loaded and preprocessed from {image_path}\")\n",
    "\n",
    "    predictions = model.predict(image_array)\n",
    "    predicted_class_index = np.argmax(predictions)\n",
    "    predicted_class_label = class_indices[predicted_class_index]\n",
    "    print(\"Prediction completed.\")\n",
    " \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(plt.imread(image_path))\n",
    "    plt.title(f\"Predicted Pose: {predicted_class_label}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    return predicted_class_label\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    model_path = \"EfficientNet_54_Augmented.h5\"\n",
    "    image_path = \"Ardha_chandrasana_test.png\"  \n",
    "    target_size = (160, 160) \n",
    "\n",
    "    class_indices = {v: k for k, v in train_generator.class_indices.items()} \n",
    "    \n",
    "\n",
    "    predicted_pose = predict_and_plot_image(model_path, image_path, target_size, class_indices)\n",
    "    print(\"Predicted Yoga Pose:\", predicted_pose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    model_path = \"EfficientNet_54_Augmented.h5\"\n",
    "    image_path = \"Ardha_chandrasana_test.png\"  \n",
    "    target_size = (160, 160) \n",
    "\n",
    "    class_indices = {v: k for k, v in train_generator.class_indices.items()}\n",
    "predicted_pose = predict_and_plot_image(model_path, image_path, target_size, class_indices)\n",
    "print(\"Predicted Yoga Pose:\", predicted_pose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Tester Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import mediapipe as mp\n",
    "\n",
    "# Paths and Parameters\n",
    "train_dir = \"path_to_train_dataset\"\n",
    "model_path = \"EfficientNet_54_Augmented.h5\"\n",
    "video_path = \"Dhanurasana_Tutorial.mp4\"  # Change to your video path\n",
    "output_path = \"Dhanurasana_Tutorial_output.mp4\"\n",
    "\n",
    "# Load Model\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "print(f\"Model loaded from {model_path}\")\n",
    "\n",
    "# Class Labels\n",
    "pose_classes = sorted([dirname for dirname in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, dirname))])\n",
    "\n",
    "print(\"Available Poses:\")\n",
    "for i, pose in enumerate(pose_classes):\n",
    "    print(f\"{i+1}. {pose}\")\n",
    "\n",
    "target_pose = input(\"Enter the name of the pose you want to try: \")\n",
    "\n",
    "# MediaPipe Pose Setup\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Preprocessing function for EfficientNet\n",
    "def preprocess_image(image, img_height=160, img_width=160):\n",
    "    image = cv2.resize(image, (img_width, img_height))\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = tf.keras.applications.efficientnet.preprocess_input(image)\n",
    "    return image\n",
    "\n",
    "# Predict Pose Function\n",
    "def check_pose(frame, target_pose):\n",
    "    # Process the frame with MediaPipe\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(frame_rgb)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        # Draw landmarks on the frame\n",
    "        mp.solutions.drawing_utils.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "        # Crop the region around the detected person\n",
    "        height, width, _ = frame.shape\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "        # Get bounding box from landmarks\n",
    "        x_min = min([lm.x for lm in landmarks]) * width\n",
    "        y_min = min([lm.y for lm in landmarks]) * height\n",
    "        x_max = max([lm.x for lm in landmarks]) * width\n",
    "        y_max = max([lm.y for lm in landmarks]) * height\n",
    "\n",
    "        x_min, y_min = max(0, int(x_min)), max(0, int(y_min))\n",
    "        x_max, y_max = min(width, int(x_max)), min(height, int(y_max))\n",
    "\n",
    "        # Extract the person region\n",
    "        roi = frame[y_min:y_max, x_min:x_max]\n",
    "        if roi.size == 0:\n",
    "            return False, 0, \"No ROI\"\n",
    "\n",
    "        # Preprocess the ROI for model prediction\n",
    "        processed_image = preprocess_image(roi)\n",
    "        predictions = model.predict(processed_image)\n",
    "        predicted_class = np.argmax(predictions)\n",
    "        predicted_pose = pose_classes[predicted_class]\n",
    "        confidence = predictions[0][predicted_class]\n",
    "\n",
    "        is_correct = predicted_pose == target_pose\n",
    "        return is_correct, confidence, predicted_pose\n",
    "\n",
    "    return False, 0, \"No Pose Detected\"\n",
    "\n",
    "# Video Processing\n",
    "if target_pose not in pose_classes:\n",
    "    print(\"Invalid pose selected.\")\n",
    "else:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Video Writer Setup\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    print(f\"Processing video for pose: {target_pose}\")\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        is_correct, confidence, predicted_pose = check_pose(frame, target_pose)\n",
    "\n",
    "        frame_display = frame.copy()\n",
    "        if predicted_pose != \"No Pose Detected\":\n",
    "            cv2.putText(frame_display, f\"Pose: {predicted_pose}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "            # Overlay green if pose matches\n",
    "            if is_correct:\n",
    "                cv2.putText(frame_display, f\"Correct Pose - Confidence: {confidence:.2f}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                height, width, _ = frame_display.shape\n",
    "                cv2.rectangle(frame_display, (50, 150), (width-50, height-50), (0, 255, 0), 2)\n",
    "            else:\n",
    "                cv2.putText(frame_display, f\"Incorrect Pose - Confidence: {confidence:.2f}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        out.write(frame_display)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Video saved as {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5721261,
     "sourceId": 9419765,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6018433,
     "sourceId": 9816298,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
